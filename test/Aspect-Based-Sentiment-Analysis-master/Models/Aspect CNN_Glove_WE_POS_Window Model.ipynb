{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GT 730M (CNMeM is disabled, cuDNN 5103)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Generated list of sentences..\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "print('Processing text dataset')\n",
    "\n",
    "tree = ET.parse(\"/home/jeet/Academics/CS671/Project/Restaurants_Train.xml\")\n",
    "corpus = tree.getroot()\n",
    "sentences = [] # List of list of sentences.\n",
    "sent = corpus.findall('.//sentence')\n",
    "for s in sent:\n",
    "    sentences.append(s.find('text').text)\n",
    "\n",
    "print ('Generated list of sentences..')\n",
    "\n",
    "MAX_SEQ_LENGTH = 69\n",
    "MAX_NB_WORDS = 40000\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B/glove.6B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the sentences into 2D Tensors and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5250 unique tokens.\n",
      "Shape of data tensor: (3044, 73)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS, lower=False)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# Here padding has been done at both the ends since we will need to take the context window size of 4 units.\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH+2, padding='post')\n",
    "data = pad_sequences(data, maxlen=MAX_SEQ_LENGTH+4, padding='pre')\n",
    "print('Shape of data tensor:', data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output tensor: (3044, 69)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:11: FutureWarning: The behavior of this method will change in future versions.  Use specific 'len(elem)' or 'elem is not None' test instead.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "raw_output = corpus.findall('.//sentence')\n",
    "train_out= np.zeros(shape=(3044,69))\n",
    "i=0\n",
    "for output in raw_output:\n",
    "    s = text_to_word_sequence(output.find('text').text, lower=False)\n",
    "    indices = np.zeros(MAX_SEQ_LENGTH)\n",
    "    \n",
    "    aspectTerms = output.find('aspectTerms')\n",
    "    if (aspectTerms):\n",
    "        aspectTerm = aspectTerms.findall('aspectTerm')\n",
    "        if (aspectTerm):\n",
    "            for aspect_term in aspectTerm:\n",
    "                try:\n",
    "                    indices[s.index(aspect_term.attrib['term'])] = 1\n",
    "#                     print (indices)\n",
    "                except:\n",
    "                    continue\n",
    "    train_out[i] = indices\n",
    "    i=i+1\n",
    "\n",
    "print (\"Shape of output tensor:\", train_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Embedding Layer set..\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "nb_words = len(word_index)\n",
    "embedding_matrix = np.zeros((nb_words + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# Here, we have set trainable = False so as to keep the embeddings fixed.\n",
    "embedding_layer = Embedding(nb_words + 1,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQ_LENGTH+4,\n",
    "                            trainable=False)\n",
    "print('Embedding Layer set..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated word Embeddings..\n",
      "Shape of Embedding_output (3044, 73, 300)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "embedding_model = Sequential()\n",
    "embedding_model.add(embedding_layer)\n",
    "\n",
    "embedding_model.compile(loss='categorical_crossentropy',\n",
    "                        optimizer='rmsprop',\n",
    "                        metrics=['acc']\n",
    "                       )\n",
    "embedding_output = embedding_model.predict(data)\n",
    "print('Generated word Embeddings..')\n",
    "print('Shape of Embedding_output', embedding_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Adding POS Tag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated Word-Embeddings and POS Tag Features..\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_input = np.zeros(shape=(3044,69,306))\n",
    "le = preprocessing.LabelEncoder()\n",
    "tags = [\"CC\",\"NN\",\"JJ\",\"VB\",\"RB\",\"IN\"]\n",
    "le.fit(tags)\n",
    "i=0\n",
    "sentences = corpus.findall('.//sentence')\n",
    "for sent in sentences:\n",
    "    s = text_to_word_sequence(sent.find('text').text)\n",
    "    tags_for_sent = nltk.pos_tag(s)\n",
    "    sent_len = len(tags_for_sent)\n",
    "    ohe = [0]*6\n",
    "\n",
    "    for j in xrange(69):\n",
    "        if j< len(tags_for_sent) and tags_for_sent[j][1][:2] in tags:\n",
    "            ohe[le.transform(tags_for_sent[j][1][:2])] = 1\n",
    "        train_input[i][j] = np.concatenate([embedding_output[i][j+2],ohe])\n",
    "    i=i+1\n",
    "    \n",
    "print('Concatenated Word-Embeddings and POS Tag Features..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WE + POS + Window Feature Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution1D, Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling1D, MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import *\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained..\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution1D(100, 5, border_mode=\"same\", input_shape=(69, 306), input_length=5))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(MaxPooling1D(pool_length=5))\n",
    "model.add(Convolution1D(50, 3, border_mode=\"same\"))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Activation(\"tanh\"))\n",
    "# softmax classifier\n",
    "model.add(Dense(69, W_regularizer=l2(0.01)))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "# model.load_weights('aspect_model_wepos.h5')\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print('Model Trained..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2739 samples, validate on 305 samples\n",
      "Epoch 1/50\n",
      "2739/2739 [==============================] - 6s - loss: 2.9188 - acc: 0.2293 - val_loss: 2.1710 - val_acc: 0.3180\n",
      "Epoch 2/50\n",
      "2739/2739 [==============================] - 6s - loss: 2.1036 - acc: 0.4045 - val_loss: 1.9510 - val_acc: 0.4951\n",
      "Epoch 3/50\n",
      "2739/2739 [==============================] - 6s - loss: 1.6313 - acc: 0.4662 - val_loss: 1.8410 - val_acc: 0.3934\n",
      "Epoch 4/50\n",
      "2739/2739 [==============================] - 6s - loss: 1.3762 - acc: 0.4947 - val_loss: 1.8268 - val_acc: 0.4164\n",
      "Epoch 5/50\n",
      "2739/2739 [==============================] - 6s - loss: 1.2554 - acc: 0.5100 - val_loss: 1.7821 - val_acc: 0.4361\n",
      "Epoch 6/50\n",
      "2739/2739 [==============================] - 6s - loss: 1.1709 - acc: 0.5020 - val_loss: 1.7582 - val_acc: 0.3934\n",
      "Epoch 7/50\n",
      "2739/2739 [==============================] - 6s - loss: 1.1240 - acc: 0.4998 - val_loss: 1.7095 - val_acc: 0.4361\n",
      "Epoch 8/50\n",
      "2739/2739 [==============================] - 6s - loss: 1.0882 - acc: 0.5086 - val_loss: 1.7785 - val_acc: 0.4492\n",
      "Epoch 9/50\n",
      "2739/2739 [==============================] - 5s - loss: 1.0468 - acc: 0.5100 - val_loss: 1.7632 - val_acc: 0.4656\n",
      "Epoch 10/50\n",
      "2739/2739 [==============================] - 6s - loss: 1.0025 - acc: 0.5177 - val_loss: 1.6809 - val_acc: 0.3836\n",
      "Epoch 11/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.9950 - acc: 0.5046 - val_loss: 1.7190 - val_acc: 0.4164\n",
      "Epoch 12/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.9744 - acc: 0.5057 - val_loss: 1.6386 - val_acc: 0.4426\n",
      "Epoch 13/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.9491 - acc: 0.5031 - val_loss: 1.7086 - val_acc: 0.4295\n",
      "Epoch 14/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.9320 - acc: 0.5133 - val_loss: 1.6809 - val_acc: 0.4262\n",
      "Epoch 15/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.9270 - acc: 0.5075 - val_loss: 1.6121 - val_acc: 0.4262\n",
      "Epoch 16/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.9019 - acc: 0.5042 - val_loss: 1.6542 - val_acc: 0.4787\n",
      "Epoch 17/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.9028 - acc: 0.5053 - val_loss: 1.6392 - val_acc: 0.4164\n",
      "Epoch 18/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8843 - acc: 0.5064 - val_loss: 1.6407 - val_acc: 0.4295\n",
      "Epoch 19/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8700 - acc: 0.5057 - val_loss: 1.6455 - val_acc: 0.4262\n",
      "Epoch 20/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8639 - acc: 0.4984 - val_loss: 1.6695 - val_acc: 0.4754\n",
      "Epoch 21/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8621 - acc: 0.5111 - val_loss: 1.6676 - val_acc: 0.4754\n",
      "Epoch 22/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8588 - acc: 0.5002 - val_loss: 1.6522 - val_acc: 0.4393\n",
      "Epoch 23/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8507 - acc: 0.5002 - val_loss: 1.6326 - val_acc: 0.4656\n",
      "Epoch 24/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8406 - acc: 0.5071 - val_loss: 1.6707 - val_acc: 0.4164\n",
      "Epoch 25/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8272 - acc: 0.5027 - val_loss: 1.6657 - val_acc: 0.4754\n",
      "Epoch 26/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8332 - acc: 0.4922 - val_loss: 1.5609 - val_acc: 0.4262\n",
      "Epoch 27/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8289 - acc: 0.5089 - val_loss: 1.6484 - val_acc: 0.4492\n",
      "Epoch 28/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8228 - acc: 0.5152 - val_loss: 1.6319 - val_acc: 0.4557\n",
      "Epoch 29/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8197 - acc: 0.4991 - val_loss: 1.6121 - val_acc: 0.4492\n",
      "Epoch 30/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8209 - acc: 0.5089 - val_loss: 1.6347 - val_acc: 0.4426\n",
      "Epoch 31/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8116 - acc: 0.5068 - val_loss: 1.6590 - val_acc: 0.4623\n",
      "Epoch 32/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8138 - acc: 0.5126 - val_loss: 1.6428 - val_acc: 0.4590\n",
      "Epoch 33/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8130 - acc: 0.5100 - val_loss: 1.5799 - val_acc: 0.4689\n",
      "Epoch 34/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8050 - acc: 0.4907 - val_loss: 1.6211 - val_acc: 0.4492\n",
      "Epoch 35/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8017 - acc: 0.5093 - val_loss: 1.6070 - val_acc: 0.4820\n",
      "Epoch 36/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7996 - acc: 0.4947 - val_loss: 1.6430 - val_acc: 0.4492\n",
      "Epoch 37/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8042 - acc: 0.5141 - val_loss: 1.5929 - val_acc: 0.4951\n",
      "Epoch 38/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7893 - acc: 0.5068 - val_loss: 1.6687 - val_acc: 0.4656\n",
      "Epoch 39/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7982 - acc: 0.5057 - val_loss: 1.5993 - val_acc: 0.4623\n",
      "Epoch 40/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7917 - acc: 0.5188 - val_loss: 1.6283 - val_acc: 0.4197\n",
      "Epoch 41/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7943 - acc: 0.5108 - val_loss: 1.6286 - val_acc: 0.4459\n",
      "Epoch 42/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7930 - acc: 0.5119 - val_loss: 1.6413 - val_acc: 0.4492\n",
      "Epoch 43/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7886 - acc: 0.5210 - val_loss: 1.6115 - val_acc: 0.4656\n",
      "Epoch 44/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7889 - acc: 0.5064 - val_loss: 1.5806 - val_acc: 0.4328\n",
      "Epoch 45/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7881 - acc: 0.5078 - val_loss: 1.6213 - val_acc: 0.4328\n",
      "Epoch 46/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7853 - acc: 0.5250 - val_loss: 1.6275 - val_acc: 0.4492\n",
      "Epoch 47/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7834 - acc: 0.5162 - val_loss: 1.6368 - val_acc: 0.4754\n",
      "Epoch 48/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7840 - acc: 0.5005 - val_loss: 1.6015 - val_acc: 0.4328\n",
      "Epoch 49/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7810 - acc: 0.5020 - val_loss: 1.6546 - val_acc: 0.4721\n",
      "Epoch 50/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7839 - acc: 0.5060 - val_loss: 1.6792 - val_acc: 0.4492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff30c4f9a50>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_input, train_out,\n",
    "          validation_split=0.1,\n",
    "          batch_size=10,\n",
    "          nb_epoch=50\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('aspect_wepos_window.h5')\n",
    "y_pred = model.predict(train_input[2739:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Window Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_output = []\n",
    "for i in xrange(y_pred.shape[0]):\n",
    "    processed_label =[]\n",
    "    for j in xrange(y_pred.shape[1]):\n",
    "        if y_pred[i][j] > 0.42:\n",
    "            processed_label.append(1)\n",
    "        else:\n",
    "            processed_label.append(0)\n",
    "    processed_output.append(processed_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = train_out[2739:]\n",
    "total_pos = 0.0\n",
    "true_pos = 0.0\n",
    "total_neg = 0.0\n",
    "true_neg = 0.0\n",
    "for i in xrange(test_data.shape[0]):\n",
    "    for j in xrange(test_data.shape[1]):\n",
    "        if test_data[i][j] == 1:\n",
    "            total_pos += 1\n",
    "            if processed_output[i][j] ==1:\n",
    "                true_pos +=1\n",
    "        if test_data[i][j] == 0:\n",
    "            total_neg += 1\n",
    "            if processed_output[i][j] ==0:\n",
    "                true_neg += 1\n",
    "\n",
    "false_pos = total_neg-true_neg\n",
    "false_neg = total_pos-true_pos\n",
    "precision = true_pos/(true_pos+false_pos)\n",
    "recall = true_pos/total_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision- 0.526530612245, recall- 0.474264705882,f1_score- 0.499032882012\n"
     ]
    }
   ],
   "source": [
    "f1_score = 2*precision*recall/(precision+recall)\n",
    "print (\"precision- \" +str(precision) + \", recall- \" +str(recall)+ \",f1_score- \" +str(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
