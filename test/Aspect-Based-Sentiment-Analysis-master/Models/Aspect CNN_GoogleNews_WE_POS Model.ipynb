{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GT 730M (CNMeM is disabled, cuDNN 5103)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Generated list of sentences..\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "print('Processing text dataset')\n",
    "\n",
    "tree = ET.parse(\"/home/jeet/Academics/CS671/Project/Restaurants_Train.xml\")\n",
    "corpus = tree.getroot()\n",
    "sentences = [] # List of list of sentences.\n",
    "sent = corpus.findall('.//sentence')\n",
    "for s in sent:\n",
    "    sentences.append(s.find('text').text)\n",
    "\n",
    "print ('Generated list of sentences..')\n",
    "\n",
    "MAX_SEQ_LENGTH = 69\n",
    "MAX_NB_WORDS = 40000\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To use GLOVE WE\n",
    "# print('Indexing word vectors.')\n",
    "\n",
    "# embeddings_index = {}\n",
    "# f = open('glove.6B/glove.6B.300d.txt')\n",
    "# for line in f:\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     embeddings_index[word] = coefs\n",
    "# f.close()\n",
    "\n",
    "# print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-81c407341fb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_NB_WORDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS, lower=False)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH, padding='post')\n",
    "print('Shape of data tensor:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3044, 69)\n",
      "(3044, 69)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:15: FutureWarning: The behavior of this method will change in future versions.  Use specific 'len(elem)' or 'elem is not None' test instead.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "raw_output = corpus.findall('.//sentence')\n",
    "train_out = []\n",
    "delet = []\n",
    "print(data.shape)\n",
    "data = np.array(data)\n",
    "print(data.shape)\n",
    "i=0\n",
    "for output in raw_output:\n",
    "    s = text_to_word_sequence(output.find('text').text, lower=True)\n",
    "    indices = np.zeros(MAX_SEQ_LENGTH)\n",
    "    \n",
    "    aspectTerms = output.find('aspectTerms')\n",
    "    if (aspectTerms):\n",
    "        aspectTerm = aspectTerms.findall('aspectTerm')\n",
    "        k=0\n",
    "        if (len(aspectTerm)>0):\n",
    "            for aspect_term in aspectTerm:\n",
    "                try:\n",
    "                    aspt = text_to_word_sequence(aspect_term.attrib['term'])\n",
    "                    if(len(aspt) < 2):\n",
    "                        indices[s.index(aspt[0])] = 1\n",
    "                    else:\n",
    "                        k=1\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "    else:\n",
    "        k=1\n",
    "    if(k==1):\n",
    "          delet.append(i)\n",
    "    train_out.append(indices)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-14f9a91f9bdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Preparing embedding matrix.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgoogle_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../GoogleNews-vectors-negative300.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# prepare embedding matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "print(data.shape)\n",
    "import gensim\n",
    "google_model = gensim.models.Word2Vec.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)\n",
    "# prepare embedding matrix\n",
    "nb_words = len(word_index)\n",
    "embedding_matrix = np.zeros((nb_words + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    if word in google_model:\n",
    "        embedding_vector = google_model[word]\n",
    "#     if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(nb_words + 1,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQ_LENGTH,\n",
    "                            trainable=False)\n",
    "print('Embedding Layer set..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "embedding_model = Sequential()\n",
    "embedding_model.add(embedding_layer)\n",
    "\n",
    "embedding_model.compile(loss='categorical_crossentropy',\n",
    "                        optimizer='rmsprop',\n",
    "                        metrics=['acc']\n",
    "                       )\n",
    "embedding_output = embedding_model.predict(data)\n",
    "print('Generated word Embeddings..')\n",
    "print('Shape of Embedding_output', embedding_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_input = np.zeros(shape=(3044,69,306))\n",
    "le = preprocessing.LabelEncoder()\n",
    "tags = [\"CC\",\"NN\",\"JJ\",\"VB\",\"RB\",\"IN\"]\n",
    "le.fit(tags)\n",
    "i=0\n",
    "sentences = corpus.findall('.//sentence')\n",
    "for sent in sentences:\n",
    "    s = text_to_word_sequence(sent.find('text').text)\n",
    "    tags_for_sent = nltk.pos_tag(s)\n",
    "    sent_len = len(tags_for_sent)\n",
    "    ohe = [0]*6\n",
    "        \n",
    "    for j in range(69):\n",
    "        if j< len(tags_for_sent) and tags_for_sent[j][1][:2] in tags:\n",
    "            ohe[le.transform(tags_for_sent[j][1][:2])] = 1\n",
    "        train_input[i][j] = np.concatenate([embedding_output[i][j],ohe])\n",
    "    i=i+1\n",
    "    \n",
    "for i in sorted(delet, reverse=True):\n",
    "    train_input = np.delete(train_input, (i), axis=0)\n",
    "    train_out = np.delete(train_out, (i), axis=0)\n",
    "print('Concatenated Word-Embeddings and POS Tag Features...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution1D, Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling1D, MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import *\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Compiling model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution1D(100, 5, border_mode=\"same\", input_shape=(69, 306)))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(MaxPooling1D(pool_length=5))\n",
    "model.add(Convolution1D(50, 3, border_mode=\"same\"))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Activation(\"tanh\"))\n",
    "# softmax classifier\n",
    "model.add(Dense(69, W_regularizer=l2(0.01)))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "# model.load_weights('aspect_model_wepos.h5')\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print('Model Compiled.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(train_input, train_out,\n",
    "          validation_split=0.1,\n",
    "          batch_size=10,\n",
    "          nb_epoch=50\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('aspect_googlenews_wepos.h5')\n",
    "y_pred = model.predict(train_input[2739:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_output = []\n",
    "for i in xrange(y_pred.shape[0]):\n",
    "    processed_label =[]\n",
    "    for j in xrange(y_pred.shape[1]):\n",
    "        if y_pred[i][j] > 0.42:\n",
    "            processed_label.append(1)\n",
    "        else:\n",
    "            processed_label.append(0)\n",
    "    processed_output.append(processed_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = train_out[2739:]\n",
    "total_pos = 0.0\n",
    "true_pos = 0.0\n",
    "total_neg = 0.0\n",
    "true_neg = 0.0\n",
    "for i in xrange(test_data.shape[0]):\n",
    "    for j in xrange(test_data.shape[1]):\n",
    "        if test_data[i][j] == 1:\n",
    "            total_pos += 1\n",
    "            if processed_output[i][j] ==1:\n",
    "                true_pos +=1\n",
    "        if test_data[i][j] == 0:\n",
    "            total_neg += 1\n",
    "            if processed_output[i][j] ==0:\n",
    "                true_neg += 1\n",
    "\n",
    "false_pos = total_neg-true_neg\n",
    "false_neg = total_pos-true_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precision = true_pos/(true_pos+false_pos)\n",
    "recall = true_pos/total_pos\n",
    "f1_score = 2*precision*recall/(precision+recall)\n",
    "print (\"precision - \" +str(precision) + \", recall- \" +str(recall)+ \", f1_score- \" +str(f1_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
